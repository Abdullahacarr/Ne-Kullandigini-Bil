{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linki Giriniz:https://www.watsons.com.tr/p/ogx-dolgunlastirici-biotin-kolajen-sampuan-385-ml-16799\n",
      "Ürün Adını Giriniz:OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\n",
      "Çok etkili \n",
      "Ürünün  dolgunlaştırıcı özelliği yanında onarıcı bir özelliği de var. Benim saçım ince telli; taranması da şekil vermesi de pek kolay değil ama biotin collagen ile harika bir sonuç aldım ve almaya devam ediyorum.,OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\n",
      "\n",
      "Ogx Dolgunlaştrıcı\n",
      "Bu şampuan gerçekten saçlarımı dolgunlaştırıyor, hatta öyle hale geldi ki çok dolgunlaştı ben de kullanmıyorum artık, zaten dalgalı bir saçım var kraliçeye dönmek istemedim doğrusu. Çok ince telli ve düz saçların kullanmasını öneririm. Üründe sorun yok benim seçimim yanlış.,OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\n",
      "\n",
      "abartıldığı gibi değil\n",
      "ilk başlarda memnunum sandım ama saçlarımı çok sertleştirdi dolgunlaştırmadı kuruttu memnun kalmadım bırakınca doğru karar verdiğimi anladım,OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\n",
      "\n",
      "Beğendim\n",
      "Şampuan gerçekten güzel saç uzatıcı ve dolgunlaştırıcı etkisi var memnunum tek bir sorun var yeni yıkayıp kuruttugumda bile bi yıkanmamış rahatsızlığı veriyor ve saçımı ertesi gün yine yıkamak zorunda oluyorum yaz için sorun yok ama çabuk yaglandırması kışın sorun olabiliyor,OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\n",
      "\n",
      "etkili değil \n",
      "herkesin saç tipi farklıdır bende güzel etki göstermedi faydasını görmedim saçımı çok sert yaptı saçları aşırı kurutuyor,OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def watsons(link,adi):\n",
    "    #watsans\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'review-balloon bg-site-light-gray my-2 rounded p-2'})\n",
    "    fo = open(\"deneme.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "    return\n",
    "def hepsiburada(link,adi):    \n",
    "    #hepsiburada\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('p',attrs={'class':'review-text'})\n",
    "   \n",
    "\n",
    "    fo = open(\"deneme.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "   \n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    \n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "    \n",
    "def suslu(link,adi):\n",
    "    #süslü sözlük\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "    url = link\n",
    "    headers={'User-Agent':user_agent,} \n",
    "\n",
    "    request=urllib.request.Request(url,None,headers) \n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read() \n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'post-body buttonable'})\n",
    "    fo = open(\"deneme.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    \n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "\n",
    "def trendyol(link,adi):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'rnr-com-cn'})\n",
    "    \n",
    "\n",
    "    fo = open(\"deneme.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    \n",
    "    for a in icerik:\n",
    "    \n",
    "         c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    \n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "link=input(\"Linki Giriniz:\")\n",
    "adi=input(\"Ürün Adını Giriniz:\")\n",
    "if(\"watsons\" in link):\n",
    "    watsons(link,adi)\n",
    "elif(\"hepsiburada\" in link):\n",
    "    hepsiburada(link,adi)\n",
    "elif(\"suslusozluk\" in link):\n",
    "    suslu(link,adi)\n",
    "elif(\"trendyol\" in link):\n",
    "    trendyol(link,adi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yorumu giriniz :Cok guzel cok begendim\n",
      "puanı giriniz : 5\n",
      "bizim tahminimiz [5.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.metrics import recall_score\n",
    "\n",
    "data = pd.read_csv(\"dosya.txt\", engine='python', encoding='utf-8')\n",
    "data = data.dropna()\n",
    "#print(data)\n",
    "\n",
    "sentences_training = [doc for doc in data.iloc[:,0]]\n",
    "classification_training = [doc for doc in data.iloc[:,2]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(analyzer='word', lowercase = True)\n",
    "\n",
    "sen_train_vector = vectorizer.fit_transform(sentences_training)\n",
    "#print(sen_train_vector.toarray())\n",
    "\n",
    "clf = GaussianNB()\n",
    "yor=input(\"yorumu giriniz :\")\n",
    "puan=input(\"puanı giriniz : \")\n",
    "\n",
    "model = clf.fit(X=sen_train_vector.toarray(), y=classification_training)\n",
    "\n",
    "sen_test_vector = vectorizer.transform([yor])\n",
    "y_pred = model.predict(sen_test_vector.toarray())\n",
    "#puan=[1]\n",
    "\n",
    "\n",
    "print(\"bizim tahminimiz %s\" % y_pred)\n",
    "#test=recall_score(puan,y_pred.shape, average='micro')\n",
    "#if(test==1):\n",
    "#    print(\"doğru bildiniz\")\n",
    "#   print(test)\n",
    "#else :\n",
    "#    print(\"yanlış bildiniz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-c7043915cb71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#filter1.filter(regex ='[(Kokusu köpürmesi güzel)]')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mset_index\u001b[1;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[0;32m   4394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4396\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None of {} are in the columns\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of ['OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı'] are in the columns\""
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"dosya.txt\", engine='python', encoding='utf-8')\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "#kategorilerdeki ürün sayısı\n",
    "#data[\"urun_adı\"].value_counts()\n",
    "#data[\"urun_adı\"].value_counts()\n",
    "#data.filter(items=['OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı'], like='bbi', axis=0)\n",
    "#data[data.urun_adı\t == \"OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı\"]\n",
    "\n",
    "#filter1 = data.loc[data['urun_adı'] == 'OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı']\n",
    "#filter1.filter(regex ='[(Kokusu köpürmesi güzel)]')\n",
    "\n",
    "data = data.set_index(['OGX Dolgunlaştırıcı Biotin&Kolejen Şampuanı'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gratis\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "url = \"https://www.gratis.com/pastel-day-long-lipcolor-kissproof-ruj/urun/Pastel1004?sku=10035782,https://www.hepsiburada.com/pastel-daylong-lipcolor-kissproof-20-likit-ruj-p-SGMAKOPST0025\"\n",
    "url_oku = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "icerik = soup.find_all('div',attrs={'class':'bv-content-summary-body-text'})\n",
    "#meta = soup.find_all('div',attrs={'class':'queue'})\n",
    "#fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "c=\"\"\n",
    "#c+=\"Sentence,NegPos\\n\"\n",
    "for a in icerik:\n",
    "    c+=a.text.strip().replace(\"  \",\"\")+'\\n\\n'\n",
    "    \n",
    "print(c)\n",
    "#fo.write(c)\n",
    "#fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)\n",
    "accuracy_score(y_true, y_pred, normalize=False)\n",
    "\n",
    "import numpy as np\n",
    "accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
