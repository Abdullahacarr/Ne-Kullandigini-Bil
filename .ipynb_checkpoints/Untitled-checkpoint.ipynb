{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linki Giriniz: https://www.suslusozluk.com/organix-biotin-collagen-%C5%9Fampuan\n",
      "Ürün Adını Giriniz:ogx biotin kollajen şampuan\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BeautifulSoup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e48e7378350b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mhepsiburada\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"suslusozluk\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0msuslu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"trendyol\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mtrendyol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e48e7378350b>\u001b[0m in \u001b[0;36msuslu\u001b[1;34m(link, adi)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0micerik\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'post-body buttonable'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dosya.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
     ]
    }
   ],
   "source": [
    "def watsons(link,adi):\n",
    "    #watsans\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'review-balloon bg-site-light-gray my-2 rounded p-2'})\n",
    "    fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "    return\n",
    "def hepsiburada(link,adi):    \n",
    "    #hepsiburada\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('p',attrs={'class':'review-text'})\n",
    "   \n",
    "\n",
    "    fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "   \n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    \n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "    \n",
    "def suslu(link,adi):\n",
    "    #süslü sözlük\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "    url = link\n",
    "    headers={'User-Agent':user_agent,} \n",
    "\n",
    "    request=urllib.request.Request(url,None,headers) \n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read() \n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'post-body buttonable'})\n",
    "    fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    \n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "\n",
    "def trendyol(link,adi):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'rnr-com-cn'})\n",
    "    \n",
    "\n",
    "    fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    \n",
    "    for a in icerik:\n",
    "    \n",
    "         c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    \n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "link=input(\"Linki Giriniz:\")\n",
    "adi=input(\"Ürün Adını Giriniz:\")\n",
    "if(\"watsons\" in link):\n",
    "    watsons(link,adi)\n",
    "elif(\"hepsiburada\" in link):\n",
    "    hepsiburada(link,adi)\n",
    "elif(\"suslusozluk\" in link):\n",
    "    suslu(link,adi)\n",
    "elif(\"trendyol\" in link):\n",
    "    trendyol(link,adi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sen_train_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-79fd9e41f8b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msen_train_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassification_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sen_train_vector' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "model = clf.fit(X=sen_train_vector.toarray(), y=classification_training)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "data = pd.read_csv(\"dosya.txt\", engine='python', encoding=\"utf-8\")\n",
    "print(data)\n",
    "\n",
    "sentences_training = [doc for doc in data.iloc[:,0]]\n",
    "classification_training = [doc for doc in data.iloc[:,1]]\n",
    "vectorizer = TfidfVectorizer(analyzer='word', lowercase = True)\n",
    "\n",
    "sen_train_vector = vectorizer.fit_transform(sentences_training)\n",
    "print(sen_train_vector.toarray())\n",
    "clf = GaussianNB()\n",
    "model = clf.fit(X=sen_train_vector.toarray(), y=classification_training)\n",
    "\n",
    "sen_test_vector = vectorizer.transform(['teşekkürler'])\n",
    "print(sen_test_vector.toarray())\n",
    "y_pred = model.predict(sen_test_vector.toarray())\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gratis\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "url = \"https://www.gratis.com/pastel-day-long-lipcolor-kissproof-ruj/urun/Pastel1004?sku=10035782,https://www.hepsiburada.com/pastel-daylong-lipcolor-kissproof-20-likit-ruj-p-SGMAKOPST0025\"\n",
    "url_oku = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "icerik = soup.find_all('div',attrs={'class':'bv-content-summary-body-text'})\n",
    "#meta = soup.find_all('div',attrs={'class':'queue'})\n",
    "#fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "c=\"\"\n",
    "#c+=\"Sentence,NegPos\\n\"\n",
    "for a in icerik:\n",
    "    c+=a.text.strip().replace(\"  \",\"\")+'\\n\\n'\n",
    "    \n",
    "print(c)\n",
    "#fo.write(c)\n",
    "#fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
