{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linki Giriniz:https://www.trendyol.com/pantene/3-u-1-arada-sampuan-ve-sac-bakim-kremi-sac-dokulmelerine-karsi-etkili-470-ml-p-238780/yorumlar?boutiqueId=370511?gclid=CjwKCAiA8qLvBRAbEiwAE_ZzPRlCuV8qUOdzskipxAJODCaehdryyL6akMYJFoiecOQBobsLnZgGGRoCsScQAvD_BwE\n",
      "√úr√ºn Adƒ±nƒ± Giriniz:pantene sa√ß bakƒ±m kremi\n",
      "fena deƒüildi,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "bilinen √ºr√ºnleri buradan uygun fiyatla almak alƒ±≈üveri≈üi kolayla≈ütƒ±rƒ±yor üëç,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "Bende kepek yaptƒ±. ƒ∞lk defa kullandƒ±ƒüƒ±m i√ßinde olabilir .,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "Fiyatƒ± uygun, √ºr√ºn kaliteli.,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "Kokusu harikaa,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "Yumu≈üacƒ±k yapƒ±yor sa√ßlarƒ±,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "paketleme gayet iyi,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "Paketleme iyi,pantene sa√ß bakƒ±m kremi\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def watsons(link,adi):\n",
    "    #watsans\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'review-balloon bg-site-light-gray my-2 rounded p-2'})\n",
    "    fo = open(\"dosya.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "    return\n",
    "def hepsiburada(link,adi):    \n",
    "    #hepsiburada\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('p',attrs={'class':'review-text'})\n",
    "   \n",
    "\n",
    "    fo = open(\"dosya.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "   \n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    \n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "    \n",
    "def suslu(link,adi):\n",
    "    #s√ºsl√º s√∂zl√ºk\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "    url = link\n",
    "    headers={'User-Agent':user_agent,} \n",
    "\n",
    "    request=urllib.request.Request(url,None,headers) \n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read() \n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'post-body buttonable'})\n",
    "    fo = open(\"dosya.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    \n",
    "    for a in icerik:\n",
    "        c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "\n",
    "def trendyol(link,adi):\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    import pandas as pd\n",
    "    url = link\n",
    "    url_oku = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "    icerik = soup.find_all('div',attrs={'class':'rnr-com-cn'})\n",
    "    \n",
    "\n",
    "    fo = open(\"dosya.txt\", \"a+\",encoding=\"utf-8\")\n",
    "    c=\"\"\n",
    "    \n",
    "    for a in icerik:\n",
    "    \n",
    "         c+=a.text.strip().replace(\"  \",\"\")+\",\"+adi+'\\n\\n'\n",
    "    \n",
    "    print(c)\n",
    "    fo.write(c)\n",
    "    fo.close()\n",
    "link=input(\"Linki Giriniz:\")\n",
    "adi=input(\"√úr√ºn Adƒ±nƒ± Giriniz:\")\n",
    "if(\"watsons\" in link):\n",
    "    watsons(link,adi)\n",
    "elif(\"hepsiburada\" in link):\n",
    "    hepsiburada(link,adi)\n",
    "elif(\"suslusozluk\" in link):\n",
    "    suslu(link,adi)\n",
    "elif(\"trendyol\" in link):\n",
    "    trendyol(link,adi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sen_train_vector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-79fd9e41f8b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msen_train_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassification_training\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sen_train_vector' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "model = clf.fit(X=sen_train_vector.toarray(), y=classification_training)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "data = pd.read_csv(\"dosya.txt\", engine='python', encoding=\"utf-8\")\n",
    "print(data)\n",
    "\n",
    "sentences_training = [doc for doc in data.iloc[:,0]]\n",
    "classification_training = [doc for doc in data.iloc[:,1]]\n",
    "vectorizer = TfidfVectorizer(analyzer='word', lowercase = True)\n",
    "\n",
    "sen_train_vector = vectorizer.fit_transform(sentences_training)\n",
    "print(sen_train_vector.toarray())\n",
    "clf = GaussianNB()\n",
    "model = clf.fit(X=sen_train_vector.toarray(), y=classification_training)\n",
    "\n",
    "sen_test_vector = vectorizer.transform(['te≈üekk√ºrler'])\n",
    "print(sen_test_vector.toarray())\n",
    "y_pred = model.predict(sen_test_vector.toarray())\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gratis\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "url = \"https://www.gratis.com/pastel-day-long-lipcolor-kissproof-ruj/urun/Pastel1004?sku=10035782,https://www.hepsiburada.com/pastel-daylong-lipcolor-kissproof-20-likit-ruj-p-SGMAKOPST0025\"\n",
    "url_oku = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(url_oku, 'html.parser')\n",
    "icerik = soup.find_all('div',attrs={'class':'bv-content-summary-body-text'})\n",
    "#meta = soup.find_all('div',attrs={'class':'queue'})\n",
    "#fo = open(\"dosya.txt\", \"w\",encoding=\"utf-8\")\n",
    "c=\"\"\n",
    "#c+=\"Sentence,NegPos\\n\"\n",
    "for a in icerik:\n",
    "    c+=a.text.strip().replace(\"  \",\"\")+'\\n\\n'\n",
    "    \n",
    "print(c)\n",
    "#fo.write(c)\n",
    "#fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
